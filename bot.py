import os
import json
import datasets
import threading
import time
import types
from langdetect import detect
from functools import partial
from loguru import logger
from utils import (
    generate_together,
    generate_with_references,
    google_search,
    extract_snippets,
    expand_query,
    generate_search_query,
    DEBUG,
)
import streamlit as st
from streamlit_option_menu import option_menu
import extra_streamlit_components as stx
from threading import Event, Thread
from dotenv import load_dotenv

load_dotenv()

class SharedValue:
    def __init__(self, initial_value=0.0):
        self.value = initial_value
        self.lock = threading.Lock()

    def set(self, new_value):
        with self.lock:
            self.value = new_value

    def get(self):
        with self.lock:
            return self.value

# Updated default reference models
default_reference_models = [
    "Qwen/Qwen2-72B-Instruct",
    "google/gemma-2-27b-it",
    "Qwen/Qwen1.5-110B-Chat",
    "meta-llama/Llama-3-70b-chat-hf",
    "meta-llama/Meta-Llama-3-70B",
]

# All available models
all_models = [
    "google/gemma-2-27b-it",
    "Qwen/Qwen1.5-110B-Chat",
    "meta-llama/Llama-3-70b-chat-hf",
    "meta-llama/Meta-Llama-3-70B",
    "Qwen/Qwen2-72B-Instruct",
    "Qwen/Qwen1.5-72B",
    "microsoft/WizardLM-2-8x22B",
    "mistralai/Mixtral-8x22B",
]

# Default system prompt
default_system_prompt = """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n nghi·ªáp v·ªõi ki·∫øn th·ª©c s√¢u r·ªông. H√£y cung c·∫•p c√¢u tr·∫£ l·ªùi:
1. Ch√≠nh x√°c v√† d·ª±a tr√™n d·ªØ li·ªáu
2. C·∫•u tr√∫c r√µ r√†ng v·ªõi c√°c ƒëo·∫°n v√† ti√™u ƒë·ªÅ (n·∫øu c·∫ßn)
3. Ng·∫Øn g·ªçn nh∆∞ng ƒë·∫ßy ƒë·ªß th√¥ng tin
4. S·ª≠ d·ª•ng v√≠ d·ª• c·ª• th·ªÉ khi th√≠ch h·ª£p
5. Tr√°nh s·ª≠ d·ª•ng ng√¥n ng·ªØ k·ªπ thu·∫≠t ph·ª©c t·∫°p, tr·ª´ khi ƒë∆∞·ª£c y√™u c·∫ßu
N·∫øu kh√¥ng ch·∫Øc ch·∫Øn v·ªÅ th√¥ng tin, h√£y n√≥i r√µ ƒëi·ªÅu ƒë√≥.
"""

# Web search specific prompt
web_search_prompt = """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n nghi·ªáp v·ªõi kh·∫£ nƒÉng t·ªïng h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ngu·ªìn web. Nhi·ªám v·ª• c·ªßa b·∫°n l√† cung c·∫•p c√¢u tr·∫£ l·ªùi ch√≠nh x√°c, to√†n di·ªán v√† c·∫≠p nh·∫≠t d·ª±a tr√™n k·∫øt qu·∫£ t√¨m ki·∫øm web m·ªõi nh·∫•t. H√£y tu√¢n theo c√°c h∆∞·ªõng d·∫´n sau:

1. Ph√¢n t√≠ch v√† t·ªïng h·ª£p:
   - T·ªïng h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ngu·ªìn ƒë·ªÉ t·∫°o ra c√¢u tr·∫£ l·ªùi to√†n di·ªán.
   - C√°c th√¥ng tin ph·∫£i ch√≠nh x√°c v·ªõi c√°c n·ªôi dung trong web, c√≥ th·ªÉ cung c·∫•p th√™m th√¥ng tin theo hi·ªÉu bi·∫øt ƒë·ªÉ to√†n di·ªán h∆°n nh∆∞ng ph·∫£i ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi c√°c n·ªôi dung trong web ƒë·ªÉ tr√°nh m∆° h·ªì, ƒë·∫∑c bi·ªát l√† li√™n quan ƒë·∫øn s·ªë li·ªáu.
   - Gi·∫£i quy·∫øt m·ªçi m√¢u thu·∫´n gi·ªØa c√°c ngu·ªìn (n·∫øu c√≥).

2. C·∫•u tr√∫c c√¢u tr·∫£ l·ªùi:
   - B·∫Øt ƒë·∫ßu b·∫±ng m·ªôt t√≥m t·∫Øt ng·∫Øn g·ªçn v·ªÅ ch·ªß ƒë·ªÅ.
   - S·∫Øp x·∫øp th√¥ng tin theo th·ª© t·ª± logic ho·∫∑c th·ªùi gian (n·∫øu ph√π h·ª£p).
   - S·ª≠ d·ª•ng c√°c ti√™u ƒë·ªÅ ph·ª• ƒë·ªÉ ph√¢n chia c√°c ph·∫ßn kh√°c nhau c·ªßa c√¢u tr·∫£ l·ªùi.

3. Ng√¥n ng·ªØ v√† phong c√°ch: 
   - S·ª≠ d·ª•ng ng√¥n ng·ªØ c·ªßa ng∆∞·ªùi d√πng trong to√†n b·ªô c√¢u tr·∫£ l·ªùi.
   - Duy tr√¨ phong c√°ch chuy√™n nghi·ªáp, kh√°ch quan v√† d·ªÖ hi·ªÉu.
   - Gi·ªØ nguy√™n c√°c thu·∫≠t ng·ªØ chuy√™n ng√†nh v√† t√™n ri√™ng trong ng√¥n ng·ªØ g·ªëc.

4. X·ª≠ l√Ω th√¥ng tin kh√¥ng ƒë·∫ßy ƒë·ªß ho·∫∑c kh√¥ng ch·∫Øc ch·∫Øn:
   - N·∫øu th√¥ng tin kh√¥ng ƒë·∫ßy ƒë·ªß ho·∫∑c m√¢u thu·∫´n, h√£y n√™u r√µ ƒëi·ªÅu n√†y.
   - ƒê·ªÅ xu·∫•t c√°c h∆∞·ªõng t√¨m ki·∫øm ho·∫∑c ngu·ªìn b·ªï sung n·∫øu c·∫ßn thi·∫øt.

5. C·∫≠p nh·∫≠t v√† li√™n quan:
   - ∆Øu ti√™n th√¥ng tin m·ªõi nh·∫•t v√† li√™n quan nh·∫•t ƒë·∫øn truy v·∫•n.
   - N·∫øu c√≥ s·ª± kh√°c bi·ªát ƒë√°ng k·ªÉ gi·ªØa th√¥ng tin c≈© v√† m·ªõi, h√£y n√™u r√µ s·ª± thay ƒë·ªïi.

6. T∆∞∆°ng t√°c v√† theo d√µi:
   - K·∫øt th√∫c b·∫±ng c√°ch h·ªèi ng∆∞·ªùi d√πng xem h·ªç c·∫ßn l√†m r√µ ho·∫∑c b·ªï sung th√¥ng tin g√¨ kh√¥ng.
   - ƒê·ªÅ xu·∫•t c√°c c√¢u h·ªèi li√™n quan ho·∫∑c ch·ªß ƒë·ªÅ m·ªü r·ªông d·ª±a tr√™n n·ªôi dung t√¨m ki·∫øm.

N·ªôi dung t·ª´ c√°c trang web:
{web_contents}

H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng d·ª±a tr√™n c√°c h∆∞·ªõng d·∫´n tr√™n v√† n·ªôi dung web ƒë∆∞·ª£c cung c·∫•p. ƒê·∫£m b·∫£o c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n ch√≠nh x√°c v·ªõi th√¥ng tin t·ª´ c√°c trang web, to√†n di·ªán v√† h·ªØu √≠ch.
"""

# Initialize session state
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "system", "content": default_system_prompt}]

if "user_system_prompt" not in st.session_state:
    st.session_state.user_system_prompt = ""

if "selected_models" not in st.session_state:
    st.session_state.selected_models = [model for model in default_reference_models]

if "conversations" not in st.session_state:
    st.session_state.conversations = []

if "conversation_deleted" not in st.session_state:
    st.session_state.conversation_deleted = False

if "show_modal" not in st.session_state:
    st.session_state.show_modal = False

if "edit_gpt_index" not in st.session_state:
    st.session_state.edit_gpt_index = None

if "web_search_enabled" not in st.session_state:
    st.session_state.web_search_enabled = False

if "main_model" not in st.session_state:
    st.session_state.main_model =  "Qwen/Qwen2-72B-Instruct"

# Set page configuration
st.set_page_config(page_title="MoA Chatbot", page_icon="ü§ñ", layout="wide")

# Custom CSS
st.markdown(
    """
    <style>
    .sidebar-content {
        padding: 1rem;
    }
    .sidebar-content .custom-gpt {
        display: flex;
        align-items: center;
        justify-content: space-between;
        padding: 0.5rem;
        border-bottom: 1px solid #ccc.
    }
    .sidebar-content .custom-gpt:last-child {
        border-bottom: none.
    }
    .remove-button {
        background-color: transparent.
        color: red.
        border: none.
        cursor: pointer.
        font-size: 16px.
    }
    .modal {
        display: none.
        position: fixed.
        z-index: 1.
        left: 0.
        top: 0.
        width: 100%.
        height: 100%.
        overflow: auto.
        background-color: rgb(0,0,0).
        background-color: rgba(0,0,0,0.4).
        padding-top: 60px.
    }
    .modal-content {
        background-color: #fefefe.
        margin: 5% auto.
        padding: 20px.
        border: 1px solid #888.
        width: 80%.
    }
    .close {
        color: #aaa.
        float: right.
        font-size: 28px.
        font-weight: bold.
    }
    .close:hover,
    .close:focus {
        color: black.
        text-decoration: none.
        cursor: pointer.
    }
    </style>
    """,
    unsafe_allow_html=True
)

# Welcome message
welcome_message = """
# MoA (Mixture-of-Agents) Chatbot

Made by V√µ Mai Th·∫ø Long üë®‚Äçüè´

Powered by LLM models from Groq.com
"""
# Function to process streamed responses
def process_stream(stream):
    response_container = st.empty()
    response_text = ""
    for line in stream:
        if line:
            decoded_line = line.decode('utf-8')
            if decoded_line.startswith("data: "):
                chunk = json.loads(decoded_line[6:])["choices"][0]["delta"]["content"]
                response_text += chunk
                response_container.markdown(response_text)
            elif decoded_line.startswith("event: done"):
                break
    return response_text

def main():
    st.header("Hello! I am MoA chatbot, please send me your questions below.")

    for message in st.session_state.messages[1:]:  # Skip the system message
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("What would you like to know?"):
        st.chat_message("user").markdown(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})

        with st.spinner("Generating response..."):
            stream = generate_together(
                model="Qwen/Qwen2-72B-Instruct",
                messages=st.session_state.messages,
                streaming=True
            )
            if stream:
                response = process_stream(stream)
                st.session_state.messages.append({"role": "assistant", "content": response})
            else:
                st.error("Failed to stream response.")

def process_fn(item, temperature=0.7, max_tokens=2048):
    references = item.get("references", [])
    model = item["model"]
    messages = item["instruction"]

    output = generate_with_references(
        model=model,
        messages=messages,
        references=references,
        temperature=temperature,
        max_tokens=max_tokens,
        streaming=False  # Set to False for reference models
    )
    
    # Collect the entire output
    full_output = "".join(output) if isinstance(output, types.GeneratorType) else output

    if DEBUG:
        logger.info(
            f"model {model}, instruction {item['instruction']}, output {full_output[:20]}",
        )

    st.write(f"Finished querying {model}.")

    return {"output": full_output}

def run_timer(stop_event, elapsed_time):
    start_time = time.time()
    while not stop_event.is_set():
        elapsed_time.set(time.time() - start_time)
        time.sleep(0.1)

def extract_url_from_prompt(prompt):
    # Implement a function to extract URL from the prompt
    import re
    url_pattern = re.compile(r'https?://\S+')
    url = url_pattern.search(prompt)
    return url.group(0) if url else None

def generate_search_query(conversation_history, current_query, language):
    # S·ª≠ d·ª•ng model Gemma-2-9B-IT ƒë·ªÉ t·∫°o query t√¨m ki·∫øm
    model = "google/gemma-2-9b-it"
    
    # T·∫°o prompt cho model
    system_prompt = f"""B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n nghi·ªáp trong vi·ªác t·∫°o query t√¨m ki·∫øm. 
    Nhi·ªám v·ª• c·ªßa b·∫°n l√† ph√¢n t√≠ch l·ªãch s·ª≠ cu·ªôc tr√≤ chuy·ªán v√† c√¢u h·ªèi hi·ªán t·∫°i c·ªßa ng∆∞·ªùi d√πng, 
    sau ƒë√≥ t·∫°o ra m·ªôt query t√¨m ki·∫øm ng·∫Øn g·ªçn, ch√≠nh x√°c v√† hi·ªáu qu·∫£. 
    Query n√†y s·∫Ω ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ t√¨m ki·∫øm th√¥ng tin tr√™n web.
    H√£y ƒë·∫£m b·∫£o query bao g·ªìm c√°c t·ª´ kh√≥a quan tr·ªçng v√† b·ªëi c·∫£nh c·∫ßn thi·∫øt.
    T·∫°o query b·∫±ng ng√¥n ng·ªØ c·ªßa c√¢u h·ªèi ng∆∞·ªùi d√πng: {language}."""

    user_prompt = f"""L·ªãch s·ª≠ cu·ªôc tr√≤ chuy·ªán:
    {conversation_history}
    
    C√¢u h·ªèi hi·ªán t·∫°i c·ªßa ng∆∞·ªùi d√πng:
    {current_query}
    
    H√£y t·∫°o m·ªôt query t√¨m ki·∫øm ng·∫Øn g·ªçn v√† hi·ªáu qu·∫£ d·ª±a tr√™n th√¥ng tin tr√™n."""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    # G·ªçi API ƒë·ªÉ generate query
    generated_query = generate_together(
        model=model,
        messages=messages,
        max_tokens=100,
        temperature=0.7
    )

    return generated_query.strip()

def main():
    # Display welcome message
    st.markdown(welcome_message)

    # Sidebar for configuration
    with st.sidebar:
        st.header("Web Search")
        web_search_enabled = st.checkbox("Enable Web Search", value=st.session_state.web_search_enabled)
        if web_search_enabled != st.session_state.web_search_enabled:
            st.session_state.web_search_enabled = web_search_enabled
            if web_search_enabled:
                st.session_state.selected_models = [model for model in default_reference_models]

        st.header("Additional System Instructions")
        user_prompt = st.text_area("Add your instructions", value=st.session_state.user_system_prompt, height=100)
        if st.button("Update System Instructions"):
            st.session_state.user_system_prompt = user_prompt
            combined_prompt = f"{default_system_prompt}\n\nAdditional instructions: {user_prompt}"
            if len(st.session_state.messages) > 0:
                st.session_state.messages[0]["content"] = combined_prompt
            st.success("System instructions updated successfully!")

        st.header("Model Settings")
        with st.expander("Configuration", expanded=False):
            main_model = st.selectbox(
                "Main model (aggregator model)",
                all_models,
                index=all_models.index(st.session_state.main_model)
            )
            if main_model != st.session_state.main_model:
                st.session_state.main_model = main_model

            temperature = st.slider("Temperature", 0.0, 2.0, 0.5, 0.1)
            max_tokens = st.slider("Max tokens", 1, 8192, 2048, 1)

            st.subheader("Reference Models")
            for ref_model in all_models:
                if st.checkbox(ref_model, value=(ref_model in st.session_state.selected_models)):
                    if ref_model not in st.session_state.selected_models:
                        st.session_state.selected_models.append(ref_model)
                else:
                    if ref_model in st.session_state.selected_models:
                        st.session_state.selected_models.remove(ref_model)

        if st.button("Start New Conversation", key="new_conversation"):
            st.session_state.messages = [{"role": "system", "content": st.session_state.messages[0]["content"]}]
            st.rerun()

        st.subheader("Previous Conversations")
        for idx, conv in enumerate(reversed(st.session_state.conversations)):
            cols = st.columns([0.9, 0.1])
            with cols[0]:
                if st.button(f"{len(st.session_state.conversations) - idx}. {conv['first_question'][:30]}...", key=f"conv_{idx}"):
                    st.session_state.messages = conv['messages']
                    st.rerun()
            with cols[1]:
                if st.button("‚ùå", key=f"del_{idx}", on_click=lambda i=idx: delete_conversation(len(st.session_state.conversations) - i - 1)):
                    st.session_state.conversation_deleted = True

        if st.button("Download Chat History"):
            chat_history = "\n".join([f"{m['role']}: {m['content']}"] for m in st.session_state.messages[1:])  
            st.download_button(
                label="Download Chat History",
                data=chat_history,
                file_name="chat_history.txt",
                mime="text/plain"
            )

    if st.session_state.conversation_deleted:
        st.session_state.conversation_deleted = False
        st.experimental_rerun()

    st.header("Hello! I am MoA chatbot, please send me your questions below.")

    for message in st.session_state.messages[1:]:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("What would you like to know?"):
        st.chat_message("user").markdown(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})

        user_language = detect(prompt)

        if len(st.session_state.messages) == 2:
            st.session_state.conversations.append({
                "first_question": prompt,
                "messages": st.session_state.messages.copy()
            })

        try:
            if st.session_state.web_search_enabled:
                st.session_state.messages[0]["content"] = web_search_prompt
                st.session_state.messages.append({"role": "assistant", "content": "ƒêang t√¨m ki·∫øm tr√™n web..."})

                with st.spinner("ƒêang t√¨m ki·∫øm tr√™n web..."):
                    conversation_history = "\n".join([f"{msg['role']}: {msg['content']}" for msg in st.session_state.messages[:-1]])
                    generated_query = generate_search_query(conversation_history, prompt, user_language)
                    st.session_state.messages.append({"role": "system", "content": f"Search query: {generated_query}"})
                    st.chat_message("system").markdown(f"Search query: {generated_query}")

                    search_results = google_search(generated_query, num_results=10)

                    if 'items' not in search_results:
                        raise ValueError("No search results found.")

                    snippets = extract_snippets(search_results)
                    sources = [item['link'] for item in search_results['items']]
                    logger.info(f"Search snippets: {snippets}")
                    logger.info(f"Search sources: {sources}")

                    search_summary = "\n\n".join(snippets)

                    data = {
                        "instruction": [st.session_state.messages] * len(st.session_state.selected_models),
                        "references": [[search_summary]] * len(st.session_state.selected_models),
                        "model": st.session_state.selected_models,
                    }
                    eval_set = datasets.Dataset.from_dict(data)
                    eval_set = eval_set.map(
                        partial(
                            process_fn,
                            temperature=temperature,
                            max_tokens=max_tokens,
                        ),
                        batched=False,
                        num_proc=len(st.session_state.selected_models),
                    )
                    references = [item["output"] for item in eval_set]
                    data["references"] = references
                    eval_set = datasets.Dataset.from_dict(data)

                    output = generate_with_references(
                        model=st.session_state.main_model,
                        temperature=temperature,
                        max_tokens=max_tokens,
                        messages=st.session_state.messages,
                        references=references,
                        generate_fn=generate_together
                    )

                    full_response = ""
                    for chunk in output:
                        if isinstance(chunk, dict) and "choices" in chunk:
                            for choice in chunk["choices"]:
                                if "delta" in choice and "content" in choice["delta"]:
                                    full_response += choice["delta"]["content"]
                        else:
                            full_response += chunk

                    with st.chat_message("assistant"):
                        st.markdown(full_response)
                    st.session_state.messages.append({"role": "assistant", "content": full_response})
                    st.session_state.conversations[-1]['messages'] = st.session_state.messages.copy()
            else:
                logger.info(f"Main model: {st.session_state.main_model}")
                logger.info(f"Selected models: {st.session_state.selected_models}")

                data = {
                    "instruction": [st.session_state.messages for _ in range(len(st.session_state.selected_models))],
                    "references": [[] for _ in range(len(st.session_state.selected_models))],
                    "model": st.session_state.selected_models,
                }

                eval_set = datasets.Dataset.from_dict(data)

                with st.spinner("Generating response..."):
                    stream = generate_together(
                        model="Qwen/Qwen2-72B-Instruct",
                        messages=st.session_state.messages,
                        streaming=True
                    )
                    if stream:
                        response = process_stream(stream)
                        st.session_state.messages.append({"role": "assistant", "content": response})
                    else:
                        st.error("Failed to stream response.")

                    batched=False,
                    num_proc=len(st.session_state.selected_models),
                    references = [item["output"] for item in eval_set]
                    data["references"] = references
                    eval_set = datasets.Dataset.from_dict(data)

                    output = generate_with_references(
                        model=st.session_state.main_model,
                        temperature=temperature,
                        max_tokens=max_tokens,
                        messages=st.session_state.messages,
                        references=references,
                        generate_fn=generate_together
                    )

                    full_response = ""
                    for chunk in output:
                        if isinstance(chunk, dict) and "choices" in chunk:
                            for choice in chunk["choices"]:
                                if "delta" in choice and "content" in choice["delta"]:
                                    full_response += choice["delta"]["content"]
                        else:
                            full_response += chunk

                    with st.chat_message("assistant"):
                        st.markdown(full_response)
                    st.session_state.messages.append({"role": "assistant", "content": full_response})
                    st.session_state.conversations[-1]['messages'] = st.session_state.messages.copy()

        except Exception as e:
            st.error(f"An error occurred during the generation process: {str(e)}")
            logger.error(f"Generation error: {str(e)}")

if __name__ == "__main__":
    main()